{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QiSZ9LjnYCC"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/07_pytorch_experiment_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "[View Source Code](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/07_pytorch_experiment_tracking.ipynb) | [View Slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/07_pytorch_experiment_tracking.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTEoiO9tnYCD"
      },
      "source": [
        "# 07. PyTorch Experiment Tracking\n",
        "\n",
        "> **Note:** This notebook uses `torchvision`'s new [multi-weight support API (available in `torchvision` v0.13+)](https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/).\n",
        "\n",
        "We've trained a fair few models now on the journey to making FoodVision Mini (an image classification model to classify images of pizza, steak or sushi).\n",
        "\n",
        "And so far we've kept track of them via Python dictionaries.\n",
        "\n",
        "Or just comparing them by the metric print outs during training.\n",
        "\n",
        "What if you wanted to run a dozen (or more) different models at once?\n",
        "\n",
        "Surely there's a better way...\n",
        "\n",
        "There is.\n",
        "\n",
        "**Experiment tracking.**\n",
        "\n",
        "And since experiment tracking is so important and integral to machine learning, you can consider this notebook your first milestone project.\n",
        "\n",
        "So welcome to Milestone Project 1: FoodVision Mini Experiment Tracking.\n",
        "\n",
        "We're going to answer the question: **how do I track my machine learning experiments?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ez84CZBnYCD"
      },
      "source": [
        "## What is experiment tracking?\n",
        "\n",
        "Machine learning and deep learning are very experimental.\n",
        "\n",
        "You have to put on your artist's beret/chef's hat to cook up lots of different models.\n",
        "\n",
        "And you have to put on your scientist's coat to track the results of various combinations of data, model architectures and training regimes.\n",
        "\n",
        "That's where **experiment tracking** comes in.\n",
        "\n",
        "If you're running lots of different experiments, **experiment tracking helps you figure out what works and what doesn't**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x-uKzPJnYCD"
      },
      "source": [
        "## Why track experiments?\n",
        "\n",
        "If you're only running a handful of models (like we've done so far), it might be okay just to track their results in print outs and a few dictionaries.\n",
        "\n",
        "However, as the number of experiments you run starts to increase, this naive way of tracking could get out of hand.\n",
        "\n",
        "So if you're following the machine learning practitioner's motto of *experiment, experiment, experiment!*, you'll want a way to track them.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/07-experiment-tracking-can-get-out-of-hand.png\" alt=\"experiment tracking can get out of hand, many different experiments with different names\" width=900/>\n",
        "\n",
        "*After building a few models and tracking their results, you'll start to notice how quickly it can get out of hand.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDJtf5GfnYCE"
      },
      "source": [
        "## Different ways to track machine learning experiments\n",
        "\n",
        "There are as many different ways to track machine learning experiments as there are experiments to run.\n",
        "\n",
        "This table covers a few.\n",
        "\n",
        "| **Method** | **Setup** | **Pros** | **Cons** | **Cost** |\n",
        "| ----- | ----- | ----- | ----- | ----- |\n",
        "| Python dictionaries, CSV files, print outs | None | Easy to setup, runs in pure Python | Hard to keep track of large numbers of experiments | Free |\n",
        "| [TensorBoard](https://www.tensorflow.org/tensorboard/get_started) | Minimal, install [`tensorboard`](https://pypi.org/project/tensorboard/) | Extensions built into PyTorch, widely recognized and used, easily scales. | User-experience not as nice as other options. | Free |\n",
        "| [Weights & Biases Experiment Tracking](https://wandb.ai/site/experiment-tracking) | Minimal, install [`wandb`](https://docs.wandb.ai/quickstart), make an account | Incredible user experience, make experiments public, tracks almost anything. | Requires external resource outside of PyTorch. | Free for personal use |\n",
        "| [MLFlow](https://mlflow.org/) | Minimal, install `mlflow` and start tracking | Fully open-source MLOps lifecycle management, many integrations. | Little bit harder to setup a remote tracking server than other services. | Free |\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/07-different-places-to-track-experiments.png\" alt=\"various places to track machine learning experiments\" width=900/>\n",
        "\n",
        "*Various places and techniques you can use to track your machine learning experiments. **Note:** There are various other options similar to Weights & Biases and open-source options similar to MLflow but I've left them out for brevity. You can find more by searching \"machine learning experiment tracking\".*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3VgtNSVnYCE"
      },
      "source": [
        "## What we're going to cover\n",
        "\n",
        "We're going to be running several different modelling experiments with various levels of data, model size and training time to try and improve on FoodVision Mini.\n",
        "\n",
        "And due to its tight integration with PyTorch and widespread use, this notebook focuses on using TensorBoard to track our experiments.\n",
        "\n",
        "However, the principles we're going to cover are similar across all of the other tools for experiment tracking.\n",
        "\n",
        "| **Topic** | **Contents** |\n",
        "| ----- | ----- |\n",
        "| **0. Getting setup** | We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. |\n",
        "| **1. Get data** | Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our FoodVision Mini model's results. |\n",
        "| **2. Create Datasets and DataLoaders** | We'll use the `data_setup.py` script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. |\n",
        "| **3. Get and customise a pretrained model** | Just like the last section, 06. PyTorch Transfer Learning we'll download a pretrained model from `torchvision.models` and customise it to our own problem. |\n",
        "| **4. Train model and track results** | Let's see what it's like to train and track the training results of a single model using TensorBoard. |\n",
        "| **5. View our model's results in TensorBoard** | Previously we visualized our model's loss curves with a helper function, now let's see what they look like in TensorBoard. |\n",
        "| **6. Creating a helper function to track experiments** | If we're going to be adhering to the machine learner practitioner's motto of *experiment, experiment, experiment!*, we best create a function that will help us save our modelling experiment results. |\n",
        "| **7. Setting up a series of modelling experiments** | Instead of running experiments one by one, how about we write some code to run several experiments at once, with different models, different amounts of data and different training times. |\n",
        "| **8. View modelling experiments in TensorBoard** | By this stage we'll have run eight modelling experiments in one go, a fair bit to keep track of, let's see what their results look like in TensorBoard. |\n",
        "| **9. Load in the best model and make predictions with it** | The point of experiment tracking is to figure out which model performs the best, let's load in the best performing model and make some predictions with it to *visualize, visualize, visualize!*. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcO9CQg6nYCE"
      },
      "source": [
        "## Where can you get help?\n",
        "\n",
        "All of the materials for this course [are available on GitHub](https://github.com/mrdbourke/pytorch-deep-learning).\n",
        "\n",
        "If you run into trouble, you can ask a question on the course [GitHub Discussions page](https://github.com/mrdbourke/pytorch-deep-learning/discussions).\n",
        "\n",
        "And of course, there's the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) and [PyTorch developer forums](https://discuss.pytorch.org/), a very helpful place for all things PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYJuVonynYCE"
      },
      "source": [
        "## 0. Getting setup\n",
        "\n",
        "Let's start by downloading all of the modules we'll need for this section.\n",
        "\n",
        "To save us writing extra code, we're going to be leveraging some of the Python scripts (such as `data_setup.py` and `engine.py`) we created in section, [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).\n",
        "\n",
        "Specifically, we're going to download the [`going_modular`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular) directory from the `pytorch-deep-learning` repository (if we don't already have it).\n",
        "\n",
        "We'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n",
        "\n",
        "`torchinfo` will help later on to give us visual summaries of our model(s).\n",
        "\n",
        "And since we're using a newer version of the `torchvision` package (v0.13 as of June 2022), we'll make sure we've got the latest versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDJvvLIAnYCE",
        "outputId": "bea42682-f30d-46a9-c6ba-21234522137b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 1.13.0.dev20220620+cu113\n",
            "torchvision version: 0.14.0.dev20220620+cu113\n"
          ]
        }
      ],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC56EukMnYCF"
      },
      "source": [
        "> **Note:** If you're using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of `torch` (0.12+) and `torchvision` (0.13+)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqfSJ6t9nYCF"
      },
      "outputs": [],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX5pN4SInYCF"
      },
      "source": [
        "Now let's setup device agnostic code.\n",
        "\n",
        "> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krz4E2umnYCF",
        "outputId": "604efc96-9b78-4022-8a13-fa54ff6c5425"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYWvtzOSnYCF"
      },
      "source": [
        "### Create a helper function to set seeds\n",
        "\n",
        "Since we've been setting random seeds a whole bunch throughout previous sections, how about we functionize it?\n",
        "\n",
        "Let's create a function to \"set the seeds\" called `set_seeds()`.\n",
        "\n",
        "> **Note:** Recalling a [random seed](https://en.wikipedia.org/wiki/Random_seed) is a way of flavouring the randomness generated by a computer. They aren't necessary to always set when running machine learning code, however, they help ensure there's an element of reproducibility (the numbers I get with my code are similar to the numbers you get with your code). Outside of an educational or experimental setting, random seeds generally aren't required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28ch-BSynYCF"
      },
      "outputs": [],
      "source": [
        "# Set seeds\n",
        "def set_seeds(seed: int=42):\n",
        "    \"\"\"Sets random sets for torch operations.\n",
        "\n",
        "    Args:\n",
        "        seed (int, optional): Random seed to set. Defaults to 42.\n",
        "    \"\"\"\n",
        "    # Set the seed for general torch operations\n",
        "    torch.manual_seed(seed)\n",
        "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
        "    torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agpW6C70nYCG"
      },
      "source": [
        "## 1. Get data\n",
        "\n",
        "As always, before we can run machine learning experiments, we'll need a dataset.\n",
        "\n",
        "We're going to continue trying to improve upon the results we've been getting on FoodVision Mini.\n",
        "\n",
        "In the previous section, [06. PyTorch Transfer Learning](https://www.learnpytorch.io/06_pytorch_transfer_learning/), we saw how powerful using a pretrained model and transfer learning could be when classifying images of pizza, steak and sushi.\n",
        "\n",
        "So how about we run some experiments and try to further improve our results?\n",
        "\n",
        "To do so, we'll use similar code to the previous section to download the [`pizza_steak_sushi.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip) (if the data doesn't already exist) except this time it's been functionalised.\n",
        "\n",
        "This will allow us to use it again later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC7U9HJfnYCG",
        "outputId": "c4e157cf-2266-49b1-ca12-5d42e5a30ef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_data(source: str,\n",
        "                  destination: str,\n",
        "                  remove_source: bool = True) -> Path:\n",
        "    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n",
        "\n",
        "    Args:\n",
        "        source (str): A link to a zipped file containing data.\n",
        "        destination (str): A target directory to unzip data to.\n",
        "        remove_source (bool): Whether to remove the source after downloading and extracting.\n",
        "\n",
        "    Returns:\n",
        "        pathlib.Path to downloaded data.\n",
        "\n",
        "    Example usage:\n",
        "        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                      destination=\"pizza_steak_sushi\")\n",
        "    \"\"\"\n",
        "    # Setup path to data folder\n",
        "    data_path = Path(\"data/\")\n",
        "    image_path = data_path / destination\n",
        "\n",
        "    # If the image folder doesn't exist, download it and prepare it...\n",
        "    if image_path.is_dir():\n",
        "        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n",
        "    else:\n",
        "        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n",
        "        image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Download pizza, steak, sushi data\n",
        "        target_file = Path(source).name\n",
        "        with open(data_path / target_file, \"wb\") as f:\n",
        "            request = requests.get(source)\n",
        "            print(f\"[INFO] Downloading {target_file} from {source}...\")\n",
        "            f.write(request.content)\n",
        "\n",
        "        # Unzip pizza, steak, sushi data\n",
        "        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
        "            print(f\"[INFO] Unzipping {target_file} data...\")\n",
        "            zip_ref.extractall(image_path)\n",
        "\n",
        "        # Remove .zip file\n",
        "        if remove_source:\n",
        "            os.remove(data_path / target_file)\n",
        "\n",
        "    return image_path\n",
        "\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xd71hkRnYCG"
      },
      "source": [
        "Excellent! Looks like we've got our pizza, steak and sushi images in standard image classification format ready to go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G66rguTnYCG"
      },
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n",
        "Now we've got some data, let's turn it into PyTorch DataLoaders.\n",
        "\n",
        "We can do so using the `create_dataloaders()` function we created in [05. PyTorch Going Modular part 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).\n",
        "\n",
        "And since we'll be using transfer learning and specifically pretrained models from [`torchvision.models`](https://pytorch.org/vision/stable/models.html), we'll create a transform to prepare our images correctly.\n",
        "\n",
        "To transform our images into tensors, we can use:\n",
        "1. Manually created transforms using `torchvision.transforms`.\n",
        "2. Automatically created transforms using `torchvision.models.MODEL_NAME.MODEL_WEIGHTS.DEFAULT.transforms()`.\n",
        "    * Where `MODEL_NAME` is a specific `torchvision.models` architecture, `MODEL_WEIGHTS` is a specific set of pretrained weights and `DEFAULT` means the \"best available weights\".\n",
        "    \n",
        "We saw an example of each of these in [06. PyTorch Transfer Learning section 2](https://www.learnpytorch.io/06_pytorch_transfer_learning/#2-create-datasets-and-dataloaders).\n",
        "\n",
        "Let's see first an example of manually creating a `torchvision.transforms` pipeline (creating a transforms pipeline this way gives the most customization but can potentially result in performance degradation if the transforms don't match the pretrained model).\n",
        "\n",
        "The main manual transformation we need to be sure of is that all of our images are normalized in ImageNet format (this is because pretrained `torchvision.models` are all pretrained on [ImageNet](https://www.image-net.org/)).\n",
        "\n",
        "We can do this with:\n",
        "\n",
        "```python\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2lsSyf5nYCG"
      },
      "source": [
        "### 2.1 Create DataLoaders using manually created transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXse4fQVnYCG",
        "outputId": "db1cb263-e9bb-44c3-cc46-2ad4be8ae748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Manually created transforms: Compose(\n",
            "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7febf1d218e0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7febf1d216a0>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup directories\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Create transform pipeline manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "print(f\"Manually created transforms: {manual_transforms}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrEgwfuxnYCG"
      },
      "source": [
        "### 2.2 Create DataLoaders using automatically created transforms\n",
        "\n",
        "Data transformed and DataLoaders created!\n",
        "\n",
        "Let's now see what the same transformation pipeline looks like but this time by using automatic transforms.\n",
        "\n",
        "We can do this by first instantiating a set of pretrained weights (for example `weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT`)  we'd like to use and calling the `transforms()` method on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riIEvdT5nYCG",
        "outputId": "80ae162f-491c-4ed2-b89f-98852b8d8a37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Automatically created transforms: ImageClassification(\n",
            "    crop_size=[224]\n",
            "    resize_size=[256]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BICUBIC\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7febf1d213a0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7febf1d21490>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup dirs\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "# Setup pretrained weights (plenty of these available in torchvision.models)\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "\n",
        "# Get transforms from weights (these are the transforms that were used to obtain the weights)\n",
        "automatic_transforms = weights.transforms()\n",
        "print(f\"Automatically created transforms: {automatic_transforms}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=automatic_transforms, # use automatic created transforms\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}